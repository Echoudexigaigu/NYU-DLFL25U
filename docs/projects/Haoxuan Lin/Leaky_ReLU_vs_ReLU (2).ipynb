{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b_-iHgaffk6"
   },
   "source": [
    "# Activation Functions in Deep CNN‚ÄìRNN Models  \n",
    "## ReLU vs. Leaky ReLU under Increasing Depth\n",
    "\n",
    "**Author:** Haoxuan Lin  \n",
    "**Course:** NYU-DLFL25U ‚Äì Introduction to Deep Learning (Undergraduate)  \n",
    "**Semester:** Fall 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A6nFhP_fwG7"
   },
   "source": [
    "## Abstract\n",
    "\n",
    "In this project, we study how the choice of activation function affects gradient flow and performance in deep neural networks. Using a controlled CNN‚ÄìRNN architecture on an MNIST digit-to-word prediction task, we compare ReLU and Leaky ReLU under increasing network depth. We show that ReLU suffers from gradient attenuation and dead activations as depth increases, while Leaky ReLU significantly stabilizes training and improves end-to-end accuracy. Our results highlight the importance of activation function choice in deep sequence prediction models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUfJXYK6N3sc"
   },
   "source": [
    "## 1. Introduction & Research Question\n",
    "\n",
    "Deep neural networks rely on nonlinear activation functions to enable expressive representations. Among them, ReLU is widely adopted due to its simplicity and favorable optimization properties. However, ReLU is known to suffer from the *dying ReLU* problem, particularly in deep networks, where neurons can become inactive and stop contributing to learning.\n",
    "\n",
    "In this project, we investigate the following research question:\n",
    "\n",
    "**How does activation function choice (ReLU vs. Leaky ReLU) affect gradient flow and performance as network depth increases in an end-to-end CNN‚ÄìRNN model?**\n",
    "\n",
    "To answer this question, we design a controlled experimental setup that isolates depth as the primary variable while keeping other architectural factors fixed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPhXVnjeGT2q"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Use this constant to decide on which device to run the training - On Colab 'cuda:0' and 'cpu' refers to GPU and CPU respectively\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "seed = 31\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM7uKdhwRhhe"
   },
   "source": [
    "## Data loading\n",
    "---\n",
    "\n",
    "This code snippet sets up data pre-processing and creates data loaders for training and testing using the MNIST data set, which is commonly used for hand-written digit classification tasks in machine learning.\n",
    "\n",
    "`DataLoader`s abstract outs the loading of a data set and iterating batches of the loaded data set. You can read the documentation to learn more about `DataLoader`s and `Dataset`s [`DataLoader`s and `Dataset`s](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfTRYSNqJwq4",
    "outputId": "9d916ea8-d06c-4f84-efad-13e099c66397"
   },
   "outputs": [],
   "source": [
    "# Batch options\n",
    "batch_size = 128  # input batch size for training\n",
    "test_batch = 1    # test batch size\n",
    "\n",
    "# Normalizes the input images for a better training\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "# Get train and test loaders to load MNIST data\n",
    "trainset = datasets.MNIST(root='.', train=True, download=True, transform=data_transform)\n",
    "testset = datasets.MNIST(root='.', train=False, download=True, transform=data_transform)\n",
    "\n",
    "# Create the dataloaders on which we can iterate to get the data in batch_size chunks\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader  = torch.utils.data.DataLoader(testset, batch_size=test_batch, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfyOh8yrQpXr"
   },
   "source": [
    "# 1 Preprocessing(no logical change, only changed to fit the expression)\n",
    "## 1.1 Auxiliary data creation\n",
    "---\n",
    "First, let's setup `labels` and `vocab` for our problem statement. For this,\n",
    "1. MNIST data set, consists of images and their corresponding labels of the form `{0, 1, 2, ‚Ä¶}`.\n",
    "First let's build a list mapping labels to `<b>` $\\phi$ `<e>`, where $\\phi \\in \\lbrace \\texttt{zero}, \\texttt{one}, \\texttt{two}, \\ldots , \\texttt{nine}\\rbrace$.\n",
    "\n",
    "2. Build a lookup table for each unique tokens in the strings. Your `vocab` list, before sorting, should look like: `['<b>', 'z', 'e', 'r', 'o', '<e>', ‚Ä¶]`.\n",
    "\n",
    "Next, complete the following two functions:\n",
    "1.   `label_to_onehot_sequence()` to convert a label to its one-hot sequence, representing the sequence of tokens. You may want to look at [`functional.one_hot` documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html) for reference.\n",
    "2. and `token_idx_to_token()` to convert the list of token indices to their corresponding string of characters.\n",
    "\n",
    "The functions below are heavily annotated so that you understand what each function is doing and how can you use them for your use case.\n",
    "The `assert` is a simple way for testing the correctness of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QPOJLA7ommbp",
    "outputId": "26c9a887-b1f1-4bcc-d97d-56fb2f8765d1"
   },
   "outputs": [],
   "source": [
    "digits = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "prefix, suffix = '<b>', '<e>'\n",
    "# Create list and Lookup Table ###########################################\n",
    "# TODOüìù: labels = list of sequences of tokens such that it converts\n",
    "#       label to digit into strings.\n",
    "# TODOüìù: vocab = list of all the unique tokens in labels.\n",
    "################################################################################\n",
    "labels = [prefix + d + suffix for d in digits]\n",
    "print(labels)\n",
    "\n",
    "tokens_unsorted = []\n",
    "for lab in labels:\n",
    "    toks = re.findall(r'<b>|<e>|[a-z]', lab)\n",
    "    for t in toks:\n",
    "        if t not in tokens_unsorted:\n",
    "            tokens_unsorted.append(t)\n",
    "\n",
    "vocab = sorted(tokens_unsorted)\n",
    "print(vocab)\n",
    "\n",
    "labelDict = {i: labels[i] for i in range(len(labels))}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "word_size = max([len(digit) for digit in digits]) + 2 # +2 for prefix and suffix\n",
    "\n",
    "# Testing that your lookup tables are setup correctly\n",
    "assert(vocab_size == 17)\n",
    "assert(len(labelDict) == 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcsxQoFnoMi0"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### DO NOT MODIFY THIS CELL #################################\n",
    "################################################################################\n",
    "# Some utility functions to check if your code is working as intended\n",
    "def assert_encoding(actual, expected):\n",
    "  assert(type(actual) == type(torch.tensor(0))) # Your output is not of type tensor, please ensure that your function should output tensors\n",
    "  if(not((expected.numpy() == actual.numpy()).all())):\n",
    "    print('expected: ', expected)\n",
    "    print('actual: ', actual)\n",
    "    assert((expected.numpy() == actual.numpy()).all())\n",
    "\n",
    "# Get the index of a token in the vocab\n",
    "def get_idx(letter):\n",
    "    return vocab.index(letter)\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODtMG2xbjydT"
   },
   "outputs": [],
   "source": [
    "# Function to convert a list of labels to a list of one-hot matrix\n",
    "def label_to_onehot_sequence(label):\n",
    "    label_index = label.item() # Get the label\n",
    "    tokens = re.findall(r'<b>|<e>|[a-z]', labels[label_index])\n",
    "    # Map label -> one-hot sequence ############################################\n",
    "    # TODOüìù: complete the function converting a single label to its corresponsing one-hot\n",
    "    # in: 9 out: one-hot('<b>nine<e>')\n",
    "    ############################################################################\n",
    "    #raise NotImplementedError(\"label_to_onehot_sequence() not implemented\")\n",
    "    idxs = [get_idx(tok) for tok in tokens]\n",
    "    idxs = torch.tensor(idxs, dtype=torch.long)\n",
    "    onehot = torch.nn.functional.one_hot(idxs, num_classes=vocab_size).to(torch.float32)\n",
    "    return onehot\n",
    "\n",
    "\n",
    "\n",
    "assert_encoding(label_to_onehot_sequence(torch.tensor(3)),\n",
    "  torch.tensor([[\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   # <b>\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],   #  t\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   #  h\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],   #  r\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   #  e\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   #  e\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], # <e>\n",
    "    dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_azynUKYsnPH"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### DO NOT MODIFY THIS CELL #################################\n",
    "################################################################################\n",
    "def batch_of_labels_to_onehot_matrix(labels):\n",
    "    # Convert labels to one-hot tensors\n",
    "    onehot_inputs = [label_to_onehot_sequence(label) for label in labels]\n",
    "\n",
    "    # Pad the length of string since, matrix operation requires fixed-size rows\n",
    "    max_len = max(len(onehot) for onehot in onehot_inputs)\n",
    "    padded_onehot = pad_sequence(onehot_inputs, batch_first=True, padding_value= 0)\n",
    "    return max_len, padded_onehot\n",
    "\n",
    "# Convert label to label onehot - used in the next to feed argmax input to RNN\n",
    "def label_to_onehot(target):\n",
    "    return F.one_hot(target, num_classes=10).to(torch.float32)\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKUMCyjYWGin"
   },
   "outputs": [],
   "source": [
    "# To convert token indices predicted by our model back to characters and form the word\n",
    "def token_idx_to_token(input):\n",
    "\n",
    "    # Convert list of token idx to '<b>œÜ<e>' ##################################\n",
    "    # TODOüìù: complete the function to convert a list of token indices to the format '<b>œÜ<e>'\n",
    "    # For each index in the input list, get the corresponding token from vocab, to build the output word\n",
    "    # example input -> [ 0, 3, 6, 13, 2,  1] -> '<b>five<e>'\n",
    "    #                    ‚¨á ‚¨á ‚¨á   ‚¨á ‚¨á  ‚¨á\n",
    "    #                   <b> f  i   v  e  <e>\n",
    "    #raise NotImplementedError(\"token_idx_to_token() not implemented\")\n",
    "    output = ''\n",
    "    for idx in input:\n",
    "        output += vocab[idx]\n",
    "    return output\n",
    "\n",
    "assert(token_idx_to_token([0, 3,  6, 13,  2,  1]) == '<b>five<e>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv1RgBq6g11d"
   },
   "source": [
    "## 1.2 Data Exploration\n",
    "---\n",
    "1. Write code to fetch a batch from the `train_loader` and display the first 10 images along with their respective sequence of tokens which will be of the form `<b>` $\\phi$ `<e>`, where $\\phi \\in \\lbrace \\texttt{zero}, \\texttt{one}, \\texttt{two}, \\ldots, \\texttt{nine}\\rbrace$.\n",
    "\n",
    "Use `matplotlib` and Jupyter notebook's visualisation capabilities. See this [PyTorch tutorial page](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) for hints on how display images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "-P9WZJC_gyoc",
    "outputId": "f25aabcf-7c7b-44de-933b-415a29757534"
   },
   "outputs": [],
   "source": [
    "# Visualise the dataset images #################################################\n",
    "# TODOüìù: Fetch a batch from the train_loader and display the first 10 images along\n",
    "#       with the label and the respective token sequence of the form <b>œÜ<e>\n",
    "################################################################################\n",
    "def visualise_images(train_loader, num_images=10):\n",
    "    images, labels_batch = next(iter(train_loader))\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        img = images[i].squeeze().numpy()\n",
    "        lbl = labels[labels_batch[i].item()]  # '<b>zero<e>' etc.\n",
    "\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(lbl)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function to visualize the images along with the label\n",
    "visualise_images(train_loader)  # Assuming `train_loader` is your DataLoader object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTIS2SX4hzNu"
   },
   "source": [
    "## Training and Validation Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnXUEOzhx-0x"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### DO NOT MODIFY THIS CELL #################################\n",
    "################################################################################\n",
    "# Utility function to print statistics for debugging as well as modifies target and output for accuracy calculation\n",
    "def trainUtility(batch_idx, epoch, output, target, padded_onehot):\n",
    "    # Printing statistics for easy debugging\n",
    "    if batch_idx == 1 or (epoch != None and epoch%500==0):\n",
    "        _, pred_idx = torch.max(output, dim=-1)\n",
    "        _, true_idx = torch.max(padded_onehot[:, 1:, :], dim=-1)\n",
    "        print('acc: ', torch.sum(pred_idx == true_idx)/(pred_idx.shape[0]*pred_idx.shape[1]))\n",
    "        print(pred_idx[0, :], true_idx[0, :])\n",
    "\n",
    "    # Converting output of size (batch_size x max_word_length-1 x vocab_size) -> ((batch_size*(max_word_length-1)) x vocab_size)\n",
    "    output =  output.view(-1, vocab_size)\n",
    "    # Converting the true labels to ((batch_size*(max_word_length-1)) x vocab_size)\n",
    "    # omitting the first character since, this is not getting predicted by the model\n",
    "    target = padded_onehot[:, 1:, :].reshape(-1, vocab_size)\n",
    "\n",
    "    return output, target\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "memn3pIvIdaN"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### DO NOT MODIFY THIS CELL #################################\n",
    "################################################################################\n",
    "# We will create a custom dataset for training our RNN in part 2.2, data will be a tuple of\n",
    "# one-hot matrix for each label (batch_size x 10) and corresponding one-hot matrix of words (batch_size x max_word_length x vocab_size)\n",
    "class RnnTrainingDataset(Dataset):\n",
    "    \"\"\"Dataset to get a tuple of one-hot matrix for each label (batch_size x 10)\n",
    "       and corresponding one-hot matrix of words (batch_size x max_word_length - 1 x vocab_size)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        \"\"\"\n",
    "        self.X = label_to_onehot(torch.tensor([x for x in range(10)]))\n",
    "        _, self.Y = batch_of_labels_to_onehot_matrix(torch.tensor([x for x in range(10)]))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10 # There are only 10 words\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx], self.Y[idx])\n",
    "\n",
    "rnn_dataset = RnnTrainingDataset()\n",
    "rnn_dataloader = torch.utils.data.DataLoader(rnn_dataset, batch_size=10, shuffle=True)\n",
    "rnn_testloader = torch.utils.data.DataLoader(rnn_dataset, batch_size=1, shuffle=True)\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGWgcXa2hyhK"
   },
   "outputs": [],
   "source": [
    "# Function which does the training for number of epochs and model and type of model passed\n",
    "def train(num_epochs, optimiser, model, dataloader=train_loader, mode='ENCODER'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    crierion = None\n",
    "    # Select Loss ##############################################################\n",
    "    # TODOüìù: criterion = loss function to classify into K classes\n",
    "    ############################################################################\n",
    "    if mode == 'ENCODER':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif mode == 'DECODER':\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    elif mode == 'E2E' :\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif mode == 'MODULAR':\n",
    "        criterion = None\n",
    "\n",
    "    lr_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "      for batch_idx, (data, target) in enumerate(dataloader):\n",
    "          data = data.to(device)\n",
    "          target = target.to(device)\n",
    "\n",
    "          if(mode == 'E2E'): # For 2.4 and 2.5 - CNN-RNN models\n",
    "            # Convert labels to one-hot tensors\n",
    "            _, padded_onehot = batch_of_labels_to_onehot_matrix(target) # Output size (batch_size x max_word_length x vocab_size)\n",
    "            padded_onehot = padded_onehot.to(device)\n",
    "\n",
    "            output = model(data, padded_onehot)      # (B, max_len-1, vocab_size)\n",
    "\n",
    "            # Forward pass #####################################################\n",
    "            # TODOüìù: output = forward pass of your model\n",
    "            ####################################################################\n",
    "            #raise NotImplementedError(\"Please implement the forward pass of your model\")\n",
    "\n",
    "            # Call trainUtility\n",
    "            output, target = trainUtility(batch_idx, epoch, output, target, padded_onehot)\n",
    "\n",
    "          elif(mode == 'ENCODER'): # For 2.1 - CNN classifier\n",
    "            output = None\n",
    "            # Forward pass #####################################################\n",
    "            # TODOüìù: output = forward pass of your model\n",
    "            ####################################################################\n",
    "            output = model(data)\n",
    "\n",
    "\n",
    "\n",
    "          elif(mode == 'DECODER'): # For 2.2 - Overfitting RNN\n",
    "            # Convert labels to one-hot tensors\n",
    "            padded_onehot = target\n",
    "\n",
    "            output = None\n",
    "            # Forward pass #####################################################\n",
    "            # TODOüìù: output = forward pass of your model, you will not pass the\n",
    "            #                  end token <e> to your model\n",
    "            ####################################################################\n",
    "            rnn_input_seq = padded_onehot[:, :-1, :]\n",
    "            output = model(data, rnn_input_seq)\n",
    "            # Call trainUtility\n",
    "            output, target = trainUtility(batch_idx, epoch, output, target, padded_onehot)\n",
    "            target_idx = torch.argmax(target, dim=1)\n",
    "\n",
    "          elif mode == 'MODULAR':\n",
    "                continue\n",
    "          else:\n",
    "                raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "          loss = None\n",
    "          # Loss computation ###################################################\n",
    "          # TODOüìù: loss = evaluate the criterion on the model output\n",
    "          ######################################################################\n",
    "          if mode == 'ENCODER':\n",
    "              loss = criterion(output, target)\n",
    "          elif mode == 'DECODER':\n",
    "                loss = criterion(output, target_idx)\n",
    "          elif mode == 'E2E':\n",
    "              logp = F.log_softmax(output, dim=1)\n",
    "              per_token_loss = -(target * logp).sum(dim=1)\n",
    "              mask = (target.sum(dim=1) > 0).float()\n",
    "              loss = (per_token_loss * mask).sum() / (mask.sum() + 1e-8)\n",
    "          else:\n",
    "              continue\n",
    "\n",
    "          # Zero grad ##########################################################\n",
    "          # TODOüìù: zero grad the parameters\n",
    "          ######################################################################\n",
    "          #raise NotImplementedError(\"Zero grad the parameters\")\n",
    "\n",
    "          optimiser.zero_grad()\n",
    "\n",
    "          # Compute gradients ##################################################\n",
    "          # TODOüìù: back-propogate loss to calculate the grad weights\n",
    "          ######################################################################\n",
    "          #raise NotImplementedError(\"Back-propogate loss to calculate the grad weights\")\n",
    "          loss.backward()\n",
    "\n",
    "\n",
    "          #New: find the zero-rate of the neuron(to see how the calculate power is wasted.)\n",
    "          with torch.no_grad():\n",
    "              grad_mean = float('nan')\n",
    "              zero_ratio = float('nan')\n",
    "\n",
    "              if hasattr(model, \"encoder\"):\n",
    "                  g1 = model.encoder.conv1.weight.grad\n",
    "                  if g1 is not None:\n",
    "                      grad_mean = g1.abs().mean().item()\n",
    "\n",
    "                  mid_act = getattr(model.encoder, \"last_mid_act\", None)\n",
    "                  if mid_act is not None and getattr(model.encoder, \"act_name\", \"\") == \"relu\":\n",
    "                      zero_ratio = (mid_act == 0).float().mean().item()\n",
    "\n",
    "              if (batch_idx + 1) % 200 == 0:\n",
    "                  log_str = f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\"\n",
    "                  if not np.isnan(grad_mean):\n",
    "                      log_str += f\" | CNN_Grad: {grad_mean:.2e} | ReLU_Dead: {zero_ratio:.2%}\"\n",
    "                  print(log_str)\n",
    "\n",
    "\n",
    "          # Optimisation step ##################################################\n",
    "          # TODOüìù: perform a single optimization step (weight update)\n",
    "          ######################################################################\n",
    "\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "          optimiser.step()\n",
    "\n",
    "          if (batch_idx+1) % 500 == 0 or (mode == 'DECODER' and epoch%1000 == 0):\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, batch_idx+1, len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFP1MB3Lhn2T"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "###################### DO NOT MODIFY THIS CELL #################################\n",
    "################################################################################\n",
    "def evaluate(model, dataloader=test_loader, mode='ENCODER'):\n",
    "    # Never forget to change model to eval mode using eval(), this freezes the model weights for updates\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in dataloader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        if(mode == 'ENCODER'):\n",
    "          # Predicted output\n",
    "          output = model(data)\n",
    "\n",
    "          # Calculate correct prediction count\n",
    "          pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "          correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        elif(mode == 'DECODER'):\n",
    "          # Predicted output\n",
    "          output = torch.tensor(model.sample(data), device=device)\n",
    "\n",
    "          # True output\n",
    "          _, true = torch.max(target, dim=-1)\n",
    "          # Remove padding\n",
    "          true = torch.tensor(np.concatenate(([0],np.delete(true.cpu().numpy(),\n",
    "                                                            np.argwhere(true.cpu().numpy()==0)))), device=device)\n",
    "\n",
    "          # Calculate correct prediction count\n",
    "          print(\"Comparing output with true:\", output.shape, true.shape)\n",
    "          correct += torch.sum(output == true)/output.shape[0]\n",
    "        elif(mode == 'MODULAR'):\n",
    "          # Predicted output\n",
    "          output = model.sample(data)\n",
    "\n",
    "          # True output\n",
    "          true = [labels[label.item()] for label in target]\n",
    "          true_indices = [0] + [get_idx(c) for c in true[0][3:-3]] + [1]\n",
    "          true = torch.tensor(true_indices)\n",
    "          output = torch.tensor(output)\n",
    "          true = torch.full(output.shape, 20) if output.shape[0] != true.shape[0] else true\n",
    "\n",
    "          # Calculate correct prediction count\n",
    "          correct += torch.sum(output == true)/output.shape[0]\n",
    "        elif(mode == 'E2E'):\n",
    "          # Predicted output\n",
    "          output = torch.tensor(model.sample(data))\n",
    "\n",
    "          # True output\n",
    "          true = [labels[label.item()] for label in target]\n",
    "          true_indices = [0] + [get_idx(c) for c in true[0][3:-3]] + [1]\n",
    "          true = torch.tensor(true_indices)\n",
    "          output = torch.tensor(output)\n",
    "          true = torch.full(output.shape, 20) if output.shape[0] != true.shape[0] else true\n",
    "\n",
    "          # Calculate correct prediction count\n",
    "          correct += torch.sum(output == true)/output.shape[0]\n",
    "\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "    print('\\nTest set: Accuracy: {:.0f}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(dataloader.dataset),\n",
    "        100 * correct / len(dataloader.dataset)))\n",
    "\n",
    "    return 100 * correct / len(dataloader.dataset)\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c3ICvb23OMI"
   },
   "source": [
    "# 2 Model Creation and Training\n",
    "## 2.1 Convolutional Network Architecture\n",
    "\n",
    "In this project, we systematically investigate the impact of **activation function design** on\n",
    "gradient propagation, neuron sparsity, and training stability in deep convolutional encoders.\n",
    "To this end, we modify the **CnnEncoder** by replacing the standard ReLU activation with different\n",
    "variants from the ReLU family, while keeping all other architectural components unchanged.\n",
    "This controlled setup allows us to isolate the effect of activation nonlinearity on model behavior.\n",
    "\n",
    "Specifically, we consider the following three ReLU-family activation functions\n",
    "used throughout the encoder.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(x) =\n",
    "\\begin{cases}\n",
    "0, & x \\le 0, \\\\\n",
    "x, & x > 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "ReLU serves as the baseline activation, inducing sparse activations but potentially causing\n",
    "gradient blockage in the negative input region.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Leaky ReLU\n",
    "\n",
    "Let $\\alpha > 0$ denote a small leakage coefficient.\n",
    "\n",
    "$$\n",
    "\\mathrm{LeakyReLU}_{\\alpha}(x) =\n",
    "\\begin{cases}\n",
    "\\alpha x, & x \\le 0, \\\\\n",
    "x, & x > 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Leaky ReLU mitigates the dead-neuron problem by allowing non-zero gradients in the negative regime,\n",
    "thereby improving gradient flow during backpropagation.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Negative-Slope ReLU (Parametric Variant)\n",
    "\n",
    "Let $\\alpha \\ge 0$ represent the magnitude of the negative slope applied to negative inputs.\n",
    "\n",
    "$$\n",
    "\\mathrm{NegSlopeReLU}_{\\alpha}(x) =\n",
    "\\begin{cases}\n",
    "- \\alpha x, & x \\le 0, \\\\\n",
    "x, & x > 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This variant explicitly inverts the sign of negative activations, introducing an alternative\n",
    "gradient routing mechanism that alters feature symmetry and may affect representational dynamics.\n",
    "\n",
    "> **Remark.**  \n",
    "> If the negative branch instead applies a positive slope ($\\alpha x$),\n",
    "> this formulation reduces to Leaky ReLU.\n",
    "> In our implementation, the sign and magnitude of the negative slope are treated\n",
    "> as explicit design choices to study their influence on optimization behavior.\n",
    "\n",
    "---\n",
    "\n",
    "By comparing these activation functions under identical training conditions, the project aims to\n",
    "analyze how activation nonlinearity shapes gradient flow, neuron utilization, and overall training\n",
    "performance in convolutional neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHT3Zskr3Nr-",
    "outputId": "9a0b8448-8a25-4e19-f934-4383b2765435"
   },
   "outputs": [],
   "source": [
    "# options\n",
    "#New: to make a more significant difference between different algos, I manually reduce the hyperparameters.\n",
    "epochs = 5         # number of epochs to train\n",
    "lr = 0.01           # learning rate\n",
    "\n",
    "cnnEncoder = None\n",
    "linearClassifier = None\n",
    "# Convolutional net architecture ###############################################\n",
    "# TODOüìù: cnnEncoder = cnn model - input: image(batch_size x 1 x img_sz x img_sz)\n",
    "#                                  output: (batch_size, 84)\n",
    "# TODOüìù: linearClassifier = linear classifier - input: (batch_size, 84)\n",
    "#                                                output: (batch_size, 10)\n",
    "################################################################################\n",
    "\n",
    "class CnnEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop-in replacement for the original encoder.\n",
    "    Keeps the same macro-structure, but inserts N middle conv layers\n",
    "    at the (B,16,11,11) resolution to control depth.\n",
    "    Output: (B,84) as before.\n",
    "    \"\"\"\n",
    "    def __init__(self, act_name='relu', negative_slope=0.01, depth=0):\n",
    "        super().__init__()\n",
    "        self.act_name = act_name\n",
    "        self.negative_slope = negative_slope\n",
    "        self.depth = int(depth)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3, stride=1)    # 1x28x28 -> 6x26x26\n",
    "        self.pool  = nn.MaxPool2d(kernel_size=2, stride=2)       # 6x26x26 -> 6x13x13\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=3, stride=1)   # 6x13x13 -> 16x11x11\n",
    "\n",
    "        # Inserted middle layers: keep shape (B,16,11,11)\n",
    "        self.mid_layers = nn.ModuleList([\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n",
    "            for _ in range(self.depth)\n",
    "        ])\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=3, stride=1) # 16x5x5 -> 120x3x3\n",
    "        self.fc1   = nn.Linear(120 * 3 * 3, 84)\n",
    "        self.norm = nn.LayerNorm(84)\n",
    "\n",
    "        # for logging (optional)\n",
    "        self.last_mid_act = None\n",
    "\n",
    "    def act(self, x):\n",
    "        if self.act_name == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.act_name == 'leaky_relu':\n",
    "            return F.leaky_relu(x, negative_slope=self.negative_slope)\n",
    "        elif self.act_name == 'neg_slope':\n",
    "            return F.leaky_relu(x, negative_slope=self.negative_slope)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown act_name: {self.act_name}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.conv1(x))   # (B,6,26,26)\n",
    "        x = self.pool(x)             # (B,6,13,13)\n",
    "        x = self.act(self.conv2(x))  # (B,16,11,11)\n",
    "\n",
    "        # Controlled depth block (the only thing you vary in main experiment)\n",
    "        for layer in self.mid_layers:\n",
    "            x = self.act(layer(x))\n",
    "        self.last_mid_act = x.detach()  # activation AFTER nonlinearity\n",
    "\n",
    "        x = self.pool(x)             # (B,16,5,5)\n",
    "        x = self.act(self.conv3(x))  # (B,120,3,3)\n",
    "        x = torch.flatten(x, 1)      # (B,1080)\n",
    "        x = self.act(self.fc1(x))    # (B,84)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "cnnEncoder = CnnEncoder()\n",
    "linearClassifier = nn.Linear(84, 10)\n",
    "\n",
    "cnnClassifier = nn.Sequential(cnnEncoder, linearClassifier)\n",
    "enc_optimiser = None\n",
    "# Creating Optimiser ###########################################################\n",
    "# TODOüìù: enc_optimiser = create a SGD optimiser for the cnnClassifier\n",
    "# TODOüìù: Fill out the train() procedure for mode='ENCODER' in the previous section\n",
    "################################################################################\n",
    "\n",
    "enc_optimiser = optim.SGD(cnnClassifier.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "train(epochs, enc_optimiser, cnnClassifier, mode='ENCODER')\n",
    "acc = evaluate(cnnClassifier, test_loader, mode='ENCODER')\n",
    "\n",
    "assert(acc > 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftoGfiIuFMh_"
   },
   "source": [
    "## 2.2 Recurrent net overfitting\n",
    "\n",
    "In this section, we train a standalone recurrent neural network to **overfit the digit-to-word mapping**.\n",
    "The goal here is **not generalization**, but rather to verify that the decoder architecture and training\n",
    "procedure are implemented correctly.\n",
    "\n",
    "Since the task consists of only ten possible digit names, a sufficiently expressive RNN should be able\n",
    "to memorize the entire mapping with near-perfect accuracy. Successful overfitting therefore serves as\n",
    "a sanity check before integrating the RNN with visual features.\n",
    "\n",
    "The decoder is implemented as an LSTM that receives, at each time step, a concatenation of:\n",
    "- the one-hot representation of the digit label, and\n",
    "- the one-hot representation of the previously generated token.\n",
    "\n",
    "During training, the end token `<e>` is excluded from the input sequence and used only as a prediction\n",
    "target. During inference, tokens are generated autoregressively starting from the begin token `<b>`\n",
    "until `<e>` is produced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Y6hyYp7FG7B"
   },
   "outputs": [],
   "source": [
    "# 1. RNN model for learning sequence of letters\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Create LSTM and Linear objects #######################################\n",
    "        # TODOüìù: self.lstm = LSTM object using input_size, hidden_size and num_layers,\n",
    "        #                     set the batch_first option to True\n",
    "        # TODOüìù: self.fc = Linear layer to convert to get final output with size\n",
    "        #                   (batch_size x vocab_size)\n",
    "        ########################################################################\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=vocab_size + input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, input, target):\n",
    "        # input is batch of one-hot labels of shape (batch_size x 10)\n",
    "        # target is one-hot of sequence of tokens without the last token and is of\n",
    "        # shape (batch_size x max_word_length-1 x vocab_size)\n",
    "\n",
    "        max_word_length = None\n",
    "\n",
    "        # Setup \n",
    "        batch_size, max_word_length, vocab_size = target.shape\n",
    "\n",
    "\n",
    "        #max_word_length += 1\n",
    "        replicated_input = None\n",
    "        rnn_input = None\n",
    "\n",
    "        # Feed-forward behaviour of the model ##################################\n",
    "        # In order to pass two inputs to our LSTM object, we need to concat input and target,\n",
    "        # we will need to change (batch_size x 10) input matrix -> (batch_size x max_word_length-1 x 10)\n",
    "        # TODOüìù: replicated_input = repeat the one-hot input (max_word_length-1)\n",
    "        #         times on 2nd dimension -> (batch_size x max_word_length-1 x 10)\n",
    "        # TODOüìù: rnn_input = concat the target with the replicated_input ->\n",
    "        #         (batch_size x max_word_length-1 x vocab_size+10)\n",
    "        replicated_input = input.unsqueeze(1).repeat(1, max_word_length, 1)\n",
    "        rnn_input = torch.cat((target, replicated_input), dim=2)\n",
    "\n",
    "        out = None\n",
    "        # TODOüìù: out = write the feed-forward behaviour of the network\n",
    "        ########################################################################\n",
    "        out, _ = self.lstm(rnn_input)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Utility function to create next input_rnn using the current token and one-hot label(input)\n",
    "    def create_concatenated_rnn_input(self, token_idx, input_digit):\n",
    "        # input is single one-hot label of shape (1 x 10)\n",
    "        # token_idx is a token index\n",
    "        rnn_input = None\n",
    "        # Create rnn_input of (1 x 1 x vocab_size+10)\n",
    "        # input_digit: (1, 10)\n",
    "        # token_idx: scalar int\n",
    "        # (1,) -> one-hot (1, vocab_size)\n",
    "        if isinstance(token_idx, torch.Tensor):\n",
    "            token_idx = token_idx.item()\n",
    "\n",
    "        token_onehot = F.one_hot(\n",
    "        torch.tensor([token_idx], device=input_digit.device),\n",
    "        num_classes=vocab_size\n",
    "        ).to(torch.float32)\n",
    "\n",
    "        rnn_input = torch.cat([token_onehot.unsqueeze(1), input_digit.unsqueeze(1)], dim=2)\n",
    "        return rnn_input\n",
    "\n",
    "    # This function samples next token given the <b> token until the <e> token\n",
    "    # This function samples next token given the <b> token until the <e> token\n",
    "\n",
    "\n",
    "    def sample(self, input):\n",
    "\n",
    "        digit_idx = torch.argmax(input, dim=1).item()  # 0~9\n",
    "\n",
    "        tokens = re.findall(r'<b>|<e>|[a-z]', labels[digit_idx])\n",
    "        max_len = len(tokens)\n",
    "\n",
    "        output = [get_idx('<b>')]\n",
    "        rnn_input = self.create_concatenated_rnn_input(get_idx('<b>'), input)  # (1,1,V+10)\n",
    "        hidden = None\n",
    "\n",
    "        for _ in range(max_len - 1):\n",
    "            out, hidden = self.lstm(rnn_input, hidden)   # out: (1,1,H)\n",
    "            out = self.fc(out)                           # (1,1,V)\n",
    "            out = out.squeeze(1)                         # (1,V)\n",
    "\n",
    "            _, pred_idx = torch.max(out, dim=-1)         # (1,)\n",
    "            idx = pred_idx.item()\n",
    "            output.append(idx)\n",
    "\n",
    "            rnn_input = self.create_concatenated_rnn_input(idx, input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8psC0whkaKa",
    "outputId": "c7775682-a4e1-4497-e466-bdf32dd9f82c"
   },
   "outputs": [],
   "source": [
    "# 2. Create an optimiser\n",
    "# options\n",
    "epochs = 5000         # number of epochs to train\n",
    "lr = 0.05             # learning rate \n",
    "hidden_size = 8    # hidden size for rnn\n",
    "embed_size = 10       # input is onehot labels\n",
    "num_layer = 1         # layers of rnn\n",
    "\n",
    "rnnModel = None\n",
    "dec_optimiser = None\n",
    "# Recurrent net overfitting ####################################################\n",
    "# TODOüìù: rnnModel = DecoderRNN() model - input: one-hot label output: one-hot matrix of words\n",
    "# TODOüìù: dec_optimiser = create a SGD optimiser object\n",
    "################################################################################\n",
    "rnnModel = DecoderRNN(input_size=embed_size,\n",
    "                      hidden_size=hidden_size,\n",
    "                      vocab_size=vocab_size,\n",
    "                      num_layers=num_layer).to(device)\n",
    "\n",
    "dec_optimiser = optim.SGD(rnnModel.parameters(), lr=lr)\n",
    "\n",
    "train(epochs, dec_optimiser, rnnModel, mode='DECODER', dataloader=rnn_dataloader)\n",
    "acc = evaluate(rnnModel, rnn_testloader, mode='DECODER')\n",
    "\n",
    "assert(acc > 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmSqHzirl2Ny"
   },
   "source": [
    "## 2.3 Modular approach\n",
    "\n",
    "In this section, we combine the previously trained CNN classifier and RNN decoder in a **modular fashion**.\n",
    "Both components are kept fixed, and no additional training is performed.\n",
    "\n",
    "Given an input image, the CNN first predicts the digit class. The predicted digit is then converted\n",
    "into a one-hot vector and passed to the RNN decoder, which generates the corresponding sequence of\n",
    "character tokens.\n",
    "\n",
    "This modular setup provides a reference point for later experiments. In particular, it allows us to\n",
    "compare:\n",
    "- fully modular inference,\n",
    "- transfer learning with frozen visual features, and\n",
    "- end-to-end trained CNN‚ÄìRNN models.\n",
    "\n",
    "Any performance gap between these approaches reflects the benefit (or limitation) of joint optimization\n",
    "across visual and sequential components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZEykWBJoXdc",
    "outputId": "6b4a4465-37dc-49db-8bb5-f11c663d04e7"
   },
   "outputs": [],
   "source": [
    "# Modular approach #############################################################\n",
    "# TODOüìù: create a class as mentioned before\n",
    "# TODOüìù: combinationModel = instantiate your created class\n",
    "################################################################################\n",
    "\n",
    "class CombinationModel(nn.Module):\n",
    "    def __init__(self, cnn, rnn): # Everything here is pretrained\n",
    "        super(CombinationModel, self).__init__()\n",
    "        self.cnn = cnn\n",
    "        self.rnn = rnn\n",
    "\n",
    "    def sample(self, image):\n",
    "        self.cnn.eval()\n",
    "        self.rnn.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.cnn(image)                 # (1,10)\n",
    "            pred_digit = torch.argmax(logits, dim=1) # (1,)\n",
    "            digit_oh = label_to_onehot(pred_digit.cpu()).to(image.device)  # (1,10)\n",
    "\n",
    "            pred_indices = self.rnn.sample(digit_oh)\n",
    "        return pred_indices\n",
    "\n",
    "combinationModel = CombinationModel(cnnClassifier, rnnModel)\n",
    "\n",
    "acc = evaluate(combinationModel, test_loader, mode='MODULAR')\n",
    "\n",
    "assert(acc > 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVh746TQo97u"
   },
   "source": [
    "## 2.4 Transfer learning\n",
    "\n",
    "We now consider a transfer learning setting in which the convolutional encoder is **frozen** and used\n",
    "to extract image embeddings, while a sequence decoder is trained on top of these embeddings.\n",
    "\n",
    "Specifically, the CNN encoder produces a fixed-length representation for each image, which is then\n",
    "concatenated with the token sequence input and fed into an LSTM-based decoder. Only the decoder\n",
    "parameters are updated during training.\n",
    "\n",
    "This setup isolates the effect of sequence learning from visual feature learning. By keeping the\n",
    "encoder fixed, we can assess how much of the final performance depends on:\n",
    "- the quality of pretrained visual representations, versus\n",
    "- joint optimization of visual and sequential components.\n",
    "\n",
    "The transfer learning model also serves as an intermediate baseline between the modular approach\n",
    "and the fully end-to-end trained architecture studied in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNs_xKALtnEX"
   },
   "outputs": [],
   "source": [
    "# 1. RNN model for learning sequence of letters\n",
    "class DecoderRNN2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN2, self).__init__()\n",
    "        # Initialisation of the class\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size + vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Concatenated vector of image embedding and one-hot of the whole\n",
    "        # sequence of tokens without the last one of size (batch_size x max_word_length-1 x vocab_size+embed_size)\n",
    "        # Feed-forward behaviour ###############################################\n",
    "        # TODOüìù: write the feed-forward behaviour of the model\n",
    "        ########################################################################\n",
    "        out, _ = self.lstm(input)       # (B,T-1,H)\n",
    "        out = self.fc(out)              # (B,T-1,V)\n",
    "        return out\n",
    "\n",
    "    def single_forward(self, input, hidden):\n",
    "        # This function is called by DigitDecoder to output a single token given\n",
    "        # the previous token along with the image embedding and the corresponding hidden tensor\n",
    "\n",
    "        # Prediction behaviour #################################################\n",
    "        # TODOüìù: do a forward pass of the model\n",
    "        # TODOüìù: return output and new hidden\n",
    "        ########################################################################\n",
    "        out, hidden = self.lstm(input, hidden)  # (1,1,H)\n",
    "        out = self.fc(out)                      # (1,1,V)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKnUTAFg_hix"
   },
   "outputs": [],
   "source": [
    "# 2. Model to predict next words given the image\n",
    "class DigitDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(DigitDecoder, self).__init__()\n",
    "        # Initialise the model #################################################\n",
    "        # TODOüìù: self.encoder = use the cnnEncoder\n",
    "        # TODOüìù: self.decoder = instantiation of DecoderRNN2\n",
    "        ########################################################################\n",
    "        self.encoder = cnnEncoder\n",
    "        self.vocab_size = num_classes\n",
    "        self.embed_size = input_size\n",
    "\n",
    "        self.decoder = DecoderRNN2(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            vocab_size=num_classes,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Feed-forward behaviour of the model ##################################\n",
    "        # TODOüìù: try to emulate forward() from section 2.2\n",
    "        # Some extra work is needed: calculate image embedding using\n",
    "        # the self.encoder\n",
    "        ########################################################################\n",
    "        img_emb = self.encoder(x)             # (B, embed_size)\n",
    "        y_in = y[:, :-1, :]                   # Target sequence WITHOUT <e> (B, T-1, V)\n",
    "        Tm1 = y_in.size(1)\n",
    "\n",
    "        img_emb_exp = img_emb.unsqueeze(1).repeat(1, Tm1, 1) # (B, T-1, embed_size)\n",
    "        dec_in = torch.cat([img_emb_exp, y_in], dim=2)        # (B, T-1, embed+V)\n",
    "\n",
    "        out = self.decoder(dec_in)            # (B, T-1, V)\n",
    "        return out\n",
    "\n",
    "    def sample(self, x):\n",
    "        # Sampling the output string one token at a time #######################\n",
    "        # TODOüìù: try to emulate sample() method from section 2.2\n",
    "        # Some extra work is needed: calculate image embedding using\n",
    "        # the self.encoder\n",
    "        ########################################################################\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            img_emb = self.encoder(x)\n",
    "            output = [get_idx('<b>')]\n",
    "            hidden = None\n",
    "            loopRun = 0\n",
    "\n",
    "            while True and loopRun < 50:\n",
    "                last_idx = output[-1]\n",
    "                token_oh = F.one_hot(\n",
    "                    torch.tensor([last_idx], device=x.device),\n",
    "                    num_classes=self.vocab_size\n",
    "                ).float()\n",
    "\n",
    "                img_step = img_emb.unsqueeze(1)\n",
    "                tok_step = token_oh.unsqueeze(1)\n",
    "\n",
    "                dec_in = torch.cat([img_step, tok_step], dim=2)\n",
    "\n",
    "                out, hidden = self.decoder.single_forward(dec_in, hidden)\n",
    "                out = out.squeeze(1)\n",
    "                _, pred_idx = torch.max(out, dim=-1)\n",
    "                idx = pred_idx.item()\n",
    "                output.append(idx)\n",
    "\n",
    "                if idx == get_idx('<e>'):\n",
    "                    break\n",
    "                loopRun += 1\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLe3qH_-xlLS",
    "outputId": "7bae7c86-2a8a-4465-c1b9-0c06332d2691"
   },
   "outputs": [],
   "source": [
    "# 3. and 4. model instantiation and optimiser creation\n",
    "epochs = 2           # number of epochs to train\n",
    "lr = 0.0001              # learning rate\n",
    "hidden_size = 64    # hidden size for rnn - set from 8, 16, 32, 64 - use the smallest one for which your model works\n",
    "num_layer = 1         # layers of rnn\n",
    "embed_size = 84\n",
    "\n",
    "tranferLearningModel = None\n",
    "tra_optimiser = None\n",
    "# Transfer learning ############################################################\n",
    "# TODOüìù: tranferLearningModel = create an object of DigitDecoder\n",
    "# TODOüìù: tra_optimiser = create a SGD optimiser object using just the DecoderRNN2 parameters\n",
    "################################################################################\n",
    "tranferLearningModel = DigitDecoder(\n",
    "    input_size=embed_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layer,\n",
    "    num_classes=vocab_size\n",
    ").to(device)\n",
    "for param in tranferLearningModel.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "tra_optimiser = optim.Adam(tranferLearningModel.decoder.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "train(epochs, tra_optimiser, tranferLearningModel, mode='E2E')\n",
    "acc = evaluate(tranferLearningModel, test_loader, mode='E2E')\n",
    "\n",
    "assert(acc > 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k94y4Bb-yhL7"
   },
   "source": [
    "## 2.5 End-to-end training\n",
    "---\n",
    "We investigate the effect of network depth and activation functions under an end-to-end CNN‚ÄìRNN training setting.\n",
    "\n",
    "Specifically, we vary the depth of the convolutional encoder by inserting additional convolutional layers between the second and third convolution blocks, while keeping all other architectural components fixed. The tested encoder depths are 0, 6, 12, and 30, corresponding to shallow, moderately deep, deep, and extremely deep plain convolutional networks, respectively.\n",
    "\n",
    "For each depth configuration, we evaluate three activation functions:\n",
    "\n",
    "ReLU,\n",
    "\n",
    "Leaky ReLU with a small positive slope,\n",
    "\n",
    "Negative-slope ReLU (non-monotonic variant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtbsQ6qMygTb",
    "outputId": "1463905a-241f-4361-8c57-f31ce53281e2"
   },
   "outputs": [],
   "source": [
    "class DigitDecoder2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes,\n",
    "                 act_name='relu', depth=0, negative_slope=0.01):\n",
    "        super(DigitDecoder2, self).__init__()\n",
    "\n",
    "        self.encoder = CnnEncoder(act_name=act_name, depth=depth, negative_slope=negative_slope)\n",
    "\n",
    "        self.vocab_size = num_classes\n",
    "        self.embed_size = input_size\n",
    "\n",
    "        self.decoder = DecoderRNN2(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            vocab_size=num_classes,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "          img_emb = self.encoder(x)                # (B, embed_size)\n",
    "          B, T, V = y.shape\n",
    "\n",
    "          y_in = y[:, :-1, :]                      # (B, T-1, V)\n",
    "          Tm1 = y_in.size(1)\n",
    "\n",
    "          img_emb_exp = img_emb.unsqueeze(1).repeat(1, Tm1, 1)  # (B, T-1, embed_size)\n",
    "          dec_in = torch.cat([img_emb_exp, y_in], dim=2)        # (B, T-1, embed_size + V)\n",
    "\n",
    "          out = self.decoder(dec_in)               # (B, T-1, V)\n",
    "          return out\n",
    "\n",
    "    def sample(self, x):\n",
    "          self.eval()\n",
    "          with torch.no_grad():\n",
    "              img_emb = self.encoder(x)            # (1, embed_size)\n",
    "\n",
    "              output = [get_idx('<b>')]\n",
    "              hidden = None\n",
    "              loopRun = 0\n",
    "\n",
    "              while True and loopRun < 50:\n",
    "                  last_idx = output[-1]\n",
    "                  token_oh = F.one_hot(\n",
    "                      torch.tensor([last_idx], device=x.device),\n",
    "                      num_classes=self.vocab_size\n",
    "                  ).float()                        # (1, V)\n",
    "\n",
    "                  img_step = img_emb.unsqueeze(1)  # (1, 1, embed_size)\n",
    "                  tok_step = token_oh.unsqueeze(1) # (1, 1, V)\n",
    "                  dec_in = torch.cat([img_step, tok_step], dim=2)  # (1, 1, embed_size + V)\n",
    "\n",
    "                  out, hidden = self.decoder.single_forward(dec_in, hidden)  # out: (1,1,V)\n",
    "                  out = out.squeeze(1)                  # (1, V)\n",
    "                  _, pred_idx = torch.max(out, dim=-1)\n",
    "                  idx = pred_idx.item()\n",
    "                  output.append(idx)\n",
    "\n",
    "                  if idx == get_idx('<e>'):\n",
    "                      break\n",
    "\n",
    "                  loopRun += 1\n",
    "\n",
    "              return output\n",
    "\n",
    "# options\n",
    "epochs = 2       # number of epochs to train - TODOüìù: set number of epochs\n",
    "lr = 0.0001           # learning rate - TODOüìù: set appropriate lr\n",
    "hidden_size = 64  # hidden size for rnn - TODOüìù: you may use the last lr\n",
    "num_layer = 1    # layers of rnn - TODOüìù: decide on number of layers to have in the rnn\n",
    "\n",
    "# End-to-end training ##########################################################\n",
    "# TODOüìù: e2eModel = create an object of DigitDetector\n",
    "# TODOüìù: e2eOptimiser = create an optimiser object using DigitDetector parameters\n",
    "################################################################################\n",
    "\n",
    "experiments = [\n",
    "    {\"name\":\"shallow_relu\",    \"act\":\"relu\",       \"depth\":0,  \"ns\":0.001},\n",
    "    {\"name\":\"deep_relu\",       \"act\":\"relu\",       \"depth\":6, \"ns\":0.001},\n",
    "    {\"name\":\"deep_leaky\",      \"act\":\"leaky_relu\", \"depth\":6, \"ns\":0.001},\n",
    "    {\"name\":\"neg_slope\",        \"act\":\"neg_slope\",        \"depth\":6, \"ns\":-0.001},\n",
    "    {\"name\":\"deep_relu\",       \"act\":\"relu\",       \"depth\":12, \"ns\":0.001},\n",
    "    {\"name\":\"deep_leaky\",      \"act\":\"leaky_relu\", \"depth\":12, \"ns\":0.001},\n",
    "    {\"name\":\"neg_slope\",        \"act\":\"neg_slope\",        \"depth\":12, \"ns\":-0.001},\n",
    "    {\"name\":\"deep_relu\",       \"act\":\"relu\",       \"depth\":30, \"ns\":0.001},\n",
    "    {\"name\":\"deep_leaky\",      \"act\":\"leaky_relu\", \"depth\":30, \"ns\":0.001},\n",
    "    {\"name\":\"neg_slope\",        \"act\":\"neg_slope\",        \"depth\":30, \"ns\":-0.001},\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for cfg in experiments:\n",
    "    exp_key = f\"{cfg['name']}_depth{cfg['depth']}\"\n",
    "\n",
    "    print(f\"\\nRunning Experiment: {exp_key}...\")\n",
    "\n",
    "    e2eModel = DigitDecoder2(\n",
    "        input_size=embed_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layer,\n",
    "        num_classes=vocab_size,\n",
    "        act_name=cfg[\"act\"],\n",
    "        depth=cfg[\"depth\"],\n",
    "        negative_slope=cfg[\"ns\"]\n",
    "    ).to(device)\n",
    "\n",
    "    e2eOptimiser = optim.Adam(e2eModel.parameters(), lr=lr)\n",
    "\n",
    "    train(epochs, e2eOptimiser, e2eModel, mode='E2E')\n",
    "\n",
    "    acc = evaluate(e2eModel, test_loader, mode='E2E')\n",
    "\n",
    "    if torch.is_tensor(acc):\n",
    "        acc = acc.item()\n",
    "    all_results[exp_key] = acc\n",
    "\n",
    "    print(f\"{exp_key} finished. Acc: {acc:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"FINAL ACCURACY SUMMARY\")\n",
    "print(\"=\"*30)\n",
    "for name, score in all_results.items():\n",
    "    print(f\"{name.ljust(20)} : {score:.2f}%\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "ai-nVtCyCQwa",
    "outputId": "5cb261e1-9242-410f-b64b-9ab3cab5dfbf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_acc(results):\n",
    "    names = list(results.keys())\n",
    "    values = list(results.values())\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(range(len(names)), values, color='skyblue', edgecolor='black', alpha=0.8)\n",
    "\n",
    "    plt.xticks([])\n",
    "\n",
    "    for bar, name, val in zip(bars, names, values):\n",
    "        x = bar.get_x() + bar.get_width() / 2\n",
    "        y = bar.get_height()\n",
    "\n",
    "        plt.text(x, y + 0.8, name, ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "        plt.text(x, 0.8, f'{val:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.ylim(0, max(values) + 8)\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Comparison of All Experiments')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_all_acc(all_results)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rUfJXYK6N3sc",
    "oM7uKdhwRhhe"
   ],
   "gpuType": "H100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
